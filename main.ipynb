{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eric's modification\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from datetime import datetime\n",
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from os import path\n",
    "\n",
    "from src.utils import *\n",
    "from src.models import *\n",
    "from src.methods import *\n",
    "\n",
    "data_path = os.path.join(os.getcwd(), 'project_data/')\n",
    "print(data_path)\n",
    "\n",
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(data_path+\"X_test.npy\")\n",
    "y_test = np.load(data_path+\"y_test.npy\")\n",
    "person_train_valid = np.load(data_path+\"person_train_valid.npy\")\n",
    "X_train_valid = np.load(data_path+\"X_train_valid.npy\")\n",
    "y_train_valid = np.load(data_path+\"y_train_valid.npy\")\n",
    "person_test = np.load(data_path+\"person_test.npy\")\n",
    "#:\\Users\\pluto\\Desktop\\2024_winter\\ECE_147\\final_project\\ECE247-Group-Project\\src\\project_data\\y_test.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(data_path + \"X_test.npy\")\n",
    "y_test = np.load(data_path + \"y_test.npy\") - 769\n",
    "person_train_valid = np.load(data_path + \"person_train_valid.npy\")\n",
    "X_train_valid = np.load(data_path + \"X_train_valid.npy\")\n",
    "y_train_valid = np.load(data_path + \"y_train_valid.npy\") - 769\n",
    "person_test = np.load(data_path + \"person_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
    "print ('Test data shape: {}'.format(X_test.shape))\n",
    "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
    "print ('Test target shape: {}'.format(y_test.shape))\n",
    "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
    "print ('Person test shape: {}'.format(person_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_valid = X_train_valid[:, np.newaxis, :, :]\n",
    "X_test = X_test[:, np.newaxis, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_valid = torch.from_numpy(X_train_valid).float()\n",
    "y_train_valid = torch.from_numpy(y_train_valid).long()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "train_valid_dataset = TensorDataset(X_train_valid, y_train_valid)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Splitting the dataset into train and valid sets\n",
    "num_train = int(0.8 * len(train_valid_dataset))\n",
    "num_valid = len(train_valid_dataset) - num_train\n",
    "train_indices, valid_indices = random_split(range(len(train_valid_dataset)), [num_train, num_valid])\n",
    "\n",
    "train_dataset = Subset(train_valid_dataset, train_indices)\n",
    "valid_dataset = Subset(train_valid_dataset, valid_indices)\n",
    "\n",
    "# Wrapping datasets with GaussianNoisyDataset\n",
    "train_dataset_noisy = GaussianNoisyDataset(train_dataset, mean=0., std=1.)\n",
    "valid_dataset_noisy = GaussianNoisyDataset(valid_dataset, mean=0., std=1.)\n",
    "test_dataset_noisy = GaussianNoisyDataset(test_dataset, mean=0., std=1.)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset_noisy, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset_noisy, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset_noisy, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Training/Valid data shape:', X_train_valid.shape)\n",
    "print('Test data shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "writer = SummaryWriter(f'runs/ConvNet+{time}')\n",
    "model = ConvNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, betas=(0.9, 0.99), eps=1e-6, weight_decay=0.0005)\n",
    "\n",
    "train(model, train_loader, valid_loader, criterion, optimizer, device=device, writer=writer, epochs=50)\n",
    "\n",
    "test_accuracy = evaluate(model, test_loader, device=device)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "writer.add_scalar('test_accuracy',test_accuracy)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "writer = SummaryWriter(f'runs/LSTM+{time}')\n",
    "lstm = LSTMModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr = 0.001, betas=(0.9, 0.99), eps=1e-6, weight_decay=0.0005)\n",
    "\n",
    "lstm = lstm.to(device)\n",
    "train(lstm, train_loader, valid_loader, criterion, optimizer, writer=writer, device=device)\n",
    "\n",
    "test_accuracy = evaluate(lstm, test_loader, device=device)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "writer = SummaryWriter(f'runs/EEG+{time}')\n",
    "eegn = EEGNet_Modified()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(eegn.parameters(), lr = 0.001, betas=(0.9, 0.99), eps=1e-7, weight_decay=0.0005)\n",
    "\n",
    "eegn = eegn.to(device)\n",
    "train(eegn, train_loader, valid_loader, criterion, optimizer, device=device, epochs=100)\n",
    "\n",
    "test_accuracy = evaluate(eegn, test_loader, device=device)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "writer = SummaryWriter(f'runs/CNN_LSTM+{time}')\n",
    "cnn_lstm = CNN_LSTM()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_lstm.parameters(), lr = 0.001, betas=(0.9, 0.99), eps=1e-6)\n",
    "\n",
    "cnn_lstm = cnn_lstm.to(device)\n",
    "train(cnn_lstm, train_loader, valid_loader, criterion, optimizer, writer=writer, device=device, epochs=50)\n",
    "\n",
    "test_accuracy = evaluate(cnn_lstm, test_loader, device=device)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "writer = SummaryWriter(f'runs/UltimateConvNet+{time}')\n",
    "ult_cnn = UltimateConvNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ult_cnn.parameters(), lr = 0.001, betas=(0.9, 0.99), eps=1e-6, weight_decay=0.0005)\n",
    "\n",
    "ult_cnn = ult_cnn.to(device)\n",
    "train(ult_cnn, train_loader, valid_loader, criterion, optimizer, device=device, writer=writer, epochs=200)\n",
    "\n",
    "test_accuracy = evaluate(ult_cnn, test_loader, device=device)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilize ray tune to find best hyperparameters\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray import train as raytrain\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from src.methods_ray import train_ray_Ultimateconfig\n",
    "\n",
    "config = {\n",
    "    'lr': tune.loguniform(1e-4, 1e-2),\n",
    "    'weight_decay': tune.loguniform(1e-4, 1e-3)\n",
    "}\n",
    "\n",
    "# Initialize Ray Tune\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "scheduler = ASHAScheduler(metric='loss', mode='min')\n",
    "reporter = CLIReporter(metric_columns=['loss'])\n",
    "\n",
    "# Execute the hyperparameter search\n",
    "analysis = tune.run(\n",
    "    train_ray_Ultimateconfig,\n",
    "    resources_per_trial={'gpu': 1},  # If you want to use GPUs, you can set {'gpu': 1} instead\n",
    "    config=config,\n",
    "    num_samples=100,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_config = analysis.get_best_config(metric='loss', mode='min')\n",
    "print('Best hyperparameters found are: ', best_config)\n",
    "\n",
    "# get best accuracy using tune config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run with best hyperparameters\n",
    "ult_cnn = UltimateConvNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ult_cnn.parameters(), lr = best_config['lr'].real, betas=(0.9, 0.99), eps=1e-6, weight_decay=best_config['weight_decay'].real)\n",
    "\n",
    "ult_cnn = ult_cnn.to(device)\n",
    "train(ult_cnn, train_loader, valid_loader, criterion, optimizer, device=device, epochs=100,patience=100)\n",
    "\n",
    "test_accuracy = evaluate(ult_cnn, test_loader, device=device)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray import train as raytrain\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from src.methods_ray import train_ray_Ultimateconfig\n",
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "scheduler = ASHAScheduler(metric='loss', mode='min', max_t= 250)\n",
    "reporter = CLIReporter(metric_columns=['loss'])\n",
    "bayesopt = BayesOptSearch(\n",
    "    metric='loss',\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# config = {\n",
    "#     'weight_decay': tune.loguniform(1e-4, 1e-3),\n",
    "# }\n",
    "\n",
    "config = {\n",
    "    'lr': tune.loguniform(1e-4, 1e-2),\n",
    "}\n",
    "\n",
    "\n",
    "analysis = tune.run(\n",
    "    train_ray_Ultimateconfig,\n",
    "    resources_per_trial={'gpu': 1},  # If you want to use GPUs, you can set {'gpu': 1} instead\n",
    "    config=config,\n",
    "    num_samples=20,\n",
    "    search_alg=bayesopt,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_config = analysis.get_best_config(metric='loss', mode='min')\n",
    "print('Best hyperparameters found are: ', best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = analysis.trial_dataframes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for d in dfs.values():\n",
    "    # TODO: Plot the results\n",
    "    plt.plot(d['training_iteration'], d['loss'], label=d['config/weight_decay'][0])\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Trial Progress Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run with best hyperparameters\n",
    "ult_cnn = UltimateConvNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ult_cnn.parameters(), lr = 0.00734, betas=(0.9, 0.99), eps=1e-6, weight_decay=0.0004370)\n",
    "\n",
    "ult_cnn = ult_cnn.to(device)\n",
    "train(ult_cnn, train_loader, valid_loader, criterion, optimizer, device=device, epochs=200,patience=40)\n",
    "\n",
    "test_accuracy = evaluate(ult_cnn, test_loader, device=device)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT(patch_size=(22, 1), num_classes=4, dim=64, num_head=8, num_layers = 2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(vit.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "vit = vit.to(device)\n",
    "train(vit, train_loader, valid_loader, criterion, optimizer, device=device, epochs=200)\n",
    "\n",
    "test_accuracy = evaluate(vit, test_loader, device=device)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eegn = EEGNet_Modified()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(eegn.parameters(), lr = 0.001, betas=(0.9, 0.99), eps=1e-7, weight_decay=0.0005)\n",
    "\n",
    "eegn = eegn.to(device)\n",
    "train(eegn, train_loader, valid_loader, criterion, optimizer, device=device, epochs=100)\n",
    "\n",
    "test_accuracy = evaluate(eegn, test_loader, device=device)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece247",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
